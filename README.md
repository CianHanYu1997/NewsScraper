# News Scraper Framework

ä¸€å€‹åŸºæ–¼ç•°æ­¥è¨­è¨ˆçš„æ–°èçˆ¬èŸ²æ¡†æ¶ï¼Œæ•´åˆ Redis ç·©å­˜èˆ‡å®šæ™‚ä»»å‹™åŠŸèƒ½ã€‚ç›®å‰åƒ…æ”¯æ´ URL çˆ¬å–ï¼ŒHTTP å…§å®¹è§£æå°šåœ¨é–‹ç™¼ä¸­ã€‚

## é–‹ç™¼ç‹€æ…‹

- âœ… ç¬¬ä¸€å±¤çˆ¬èŸ²ï¼šURL æ”¶é›†ï¼ˆå·²å®Œæˆï¼‰
- ğŸš§ ç¬¬äºŒå±¤çˆ¬èŸ²ï¼šå…§å®¹è§£æï¼ˆé–‹ç™¼ä¸­ï¼‰
## ç‰¹è‰²

- ç•°æ­¥çˆ¬èŸ²è¨­è¨ˆ (ä½¿ç”¨ `asyncio`)
- å®šæ™‚ä»»å‹™æ’ç¨‹ (Celery)
- Redis ç·©å­˜ç®¡ç†
- ä»£ç†æ± æ”¯æ´
- æ¨¡çµ„åŒ–è¨­è¨ˆ
- å¤šé‡æ–°èä¾†æºæ”¯æ´
- éˆæ´»çš„é é¢è¼‰å…¥ç­–ç•¥

## ç³»çµ±éœ€æ±‚

### åŸºç¤ç’°å¢ƒ
- Python 3.7+
- Redis Server
- Chrome/Chromium
- ChromeDriver

### Python å¥—ä»¶
```
selenium
beautifulsoup4
celery
redis
eventlet    # Windows ç’°å¢ƒä¸‹ Celery éœ€è¦
httpx
fake-useragent
```

## å°ˆæ¡ˆçµæ§‹

```
â”œâ”€â”€ scrapers/               # çˆ¬èŸ²å¯¦ç¾
â”‚   â”œâ”€â”€ first_layer/        # ç¬¬ä¸€å±¤çˆ¬èŸ² (URLæ”¶é›†)
â”‚   â”œâ”€â”€ second_layer/       # ç¬¬äºŒå±¤çˆ¬èŸ² (å…§å®¹è§£æ)
â”‚   â””â”€â”€ base.py             # çˆ¬èŸ²åŸºç¤é¡
â”œâ”€â”€ managers/               # ç®¡ç†æ¨¡çµ„
â”œâ”€â”€ models/                 # æ•¸æ“šæ¨¡å‹
â”œâ”€â”€ strategies/             # çˆ¬èŸ²ç­–ç•¥
â”‚   â””â”€â”€ page_load.py        # é é¢è¼‰å…¥ç­–ç•¥
â”œâ”€â”€ utils/                  # å·¥å…·é¡
â”‚   â”œâ”€â”€ redis_client.py     # Redisæ“ä½œ
â”‚   â””â”€â”€ proxy_operations.py # ä»£ç†æ± æ“ä½œ
â”œâ”€â”€ celery_scraper/         # Celeryä»»å‹™
â””â”€â”€ config/                 # é…ç½®æ–‡ä»¶
```
## æ ¸å¿ƒçµ„ä»¶

### 1. çˆ¬èŸ²åŸºç¤æ¶æ§‹ (Base Scrapers)

æ¡†æ¶æä¾›å…©ç¨®åŸºç¤çˆ¬èŸ²é¡å‹ï¼š

#### NewsSeleniumFetcher (SeleniumåŸºç¤çˆ¬èŸ²)
- è™•ç†å‹•æ…‹åŠ è¼‰å…§å®¹çš„ç¶²ç«™
- æ”¯æ´å¤šç¨®é é¢è¼‰å…¥ç­–ç•¥
- ç•°æ­¥æ“ä½œè¨­è¨ˆ
- ä¸»è¦åŠŸèƒ½ï¼š
  - URLæ¡é›†
  - é é¢å°èˆª
  - å‹•æ…‹å…§å®¹è™•ç†

é—œéµæ–¹æ³•ï¼š
```python
class NewsSeleniumFetcher(ABC):
    @abstractmethod
    def get_name(self) -> str:
        """è¿”å›æ–°èç¶²ç«™åç¨±"""
        pass

    @abstractmethod
    def get_base_url(self) -> str:
        """è¿”å›æ–°èç¶²ç«™çš„åŸºç¤URL"""
        pass

    @abstractmethod
    def get_url_elements_locator(self) -> tuple:
        """è¿”å›URLå…ƒç´ çš„å®šä½å™¨"""
        pass

    async def fetch_urls(self, load_count: Optional[int] = None) -> List[str]:
        """ç²å–æ–°èURLåˆ—è¡¨"""
        pass
```

#### NewsHTTPFetcher (HTTPåŸºç¤çˆ¬èŸ²)
- è™•ç†éœæ…‹å…§å®¹çš„ç¶²ç«™
- æ”¯æ´ä»£ç†æ± æ•´åˆ
- ç•°æ­¥HTTPè«‹æ±‚
- ä¸»è¦åŠŸèƒ½ï¼š
  - å…§å®¹è§£æ
  - å…ƒæ•¸æ“šæå–
  - æ–°èè³‡è¨Šçµæ§‹åŒ–

### 2. çˆ¬èŸ²ç®¡ç†å™¨ (ScraperManager)
- çµ±ä¸€ç®¡ç†å¤šå€‹çˆ¬èŸ²å¯¦ä¾‹
- æ”¯æ´ç•°æ­¥çˆ¬å–
- éŒ¯èª¤è™•ç†èˆ‡æ—¥èªŒè¨˜éŒ„

### 3. Redis å®¢æˆ¶ç«¯
- URL å»é‡
- ä»»å‹™ç‹€æ…‹è¿½è¹¤
- çµ±è¨ˆè³‡è¨Šå„²å­˜

### 4. ä»£ç†æ± ç®¡ç†
- æ”¯æ´ä»£ç†ä¼ºæœå™¨ä½¿ç”¨
- ä»£ç†è‡ªå‹•åˆ‡æ›
- ä»£ç†å¯ç”¨æ€§é©—è­‰

### 5. å®šæ™‚ä»»å‹™
- ä½¿ç”¨ Celery é€²è¡Œä»»å‹™æ’ç¨‹
- å®šæœŸåŸ·è¡Œçˆ¬èŸ²ä»»å‹™

## é é¢è¼‰å…¥ç­–ç•¥

æ”¯æ´ä¸‰ç¨®é é¢è¼‰å…¥ç­–ç•¥ï¼š

1. ScrollLoadStrategyï¼šæ»¾å‹•åŠ è¼‰
2. PaginationLoadStrategyï¼šåˆ†é åŠ è¼‰
3. ScrollPaginationLoadStrategyï¼šæ··åˆåŠ è¼‰ç­–ç•¥

```python
from strategies.page_load import ScrollLoadStrategy
scraper = NewsSeleniumFetcher(
    page_load_strategy=ScrollLoadStrategy(scroll_pause_time=2.0)
)
```

## å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒæº–å‚™

ç¢ºä¿å·²å®‰è£æ‰€æœ‰å¿…è¦çš„å¥—ä»¶ï¼š
```bash
pip install -r requirements.txt
```

### 2. å•Ÿå‹•æœå‹™

```bash
# å•Ÿå‹• Redis æœå‹™
redis-server

# Windows ç’°å¢ƒä¸‹å•Ÿå‹• Celery Worker
celery -A celery_scraper worker -l info -P eventlet

# å•Ÿå‹• Celery Beat (å®šæ™‚ä»»å‹™)
celery -A celery_scraper beat -l info
```

### 3. åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from managers.scraper_manager import ScraperManager
from scrapers.first_layer.setn_crawler import SETNScraper
from utils.redis_client import RedisClient

async def main():
    # åˆå§‹åŒ– Redis å®¢æˆ¶ç«¯
    redis_client = RedisClient()
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = ScraperManager()
    
    # è¨»å†Šçˆ¬èŸ²
    manager.register_scraper(SETNScraper())
    
    # åŸ·è¡Œçˆ¬èŸ²ä¸¦ç²å–çµæœ
    urls = await manager.scrape_all()
    stats = await redis_client.add_urls(urls)
    print(f"çˆ¬èŸ²çµ±è¨ˆ: {stats}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Redis é…ç½®

```python
redis_config = {
    'host': 'localhost',
    'port': 6379,
    'db_crawler': 0  # çˆ¬èŸ²æ•¸æ“šåº«
}
```

## Windows ç’°å¢ƒæ³¨æ„äº‹é …

1. Windows ç’°å¢ƒä¸‹å¿…é ˆä½¿ç”¨ eventlet ä½œç‚º Celery çš„åŸ·è¡Œæ± 
2. éœ€è¦å®‰è£ eventletï¼š`pip install eventlet`
3. å•Ÿå‹• Celery æ™‚éœ€æŒ‡å®š eventletï¼š`-P eventlet`

## ä¸€èˆ¬æ³¨æ„äº‹é …

1. ç¢ºä¿ Redis æœå‹™å·²å•Ÿå‹•
2. é©ç•¶è¨­ç½®çˆ¬èŸ²é–“éš”ï¼Œé¿å…å°ç›®æ¨™ç¶²ç«™é€ æˆè² æ“”
3. ç¢ºä¿ ChromeDriver ç‰ˆæœ¬èˆ‡ Chrome ç€è¦½å™¨ç‰ˆæœ¬ç›¸ç¬¦
4. æ³¨æ„è¨˜æ†¶é«”ä½¿ç”¨ï¼Œç‰¹åˆ¥æ˜¯åœ¨å¤§é‡çˆ¬å–æ™‚
